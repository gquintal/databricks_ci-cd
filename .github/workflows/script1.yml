name: Dynamic Databricks Notebook Deploy 3
on:
  push:
    branches:
      - develop

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:

    - name: Checkout Repository
      uses: actions/checkout@v3
    
    - name: Install jq & curl
      run: sudo apt-get update && sudo apt-get install -y jq curl
    
    - name: Export multiple notebooks (raw)
      run: |
        ORIGIN_HOST=${{ secrets.DATABRICKS_ORIGIN_HOST }}
        ORIGIN_TOKEN=${{ secrets.DATABRICKS_ORIGIN_TOKEN }}
        NOTEBOOK_BASE="/Workspace/Users/certificaciones@lattevida.onmicrosoft.com/source_ntbk"
          NOTEBOOKS=("ntbk_1" "ntbk_2")  # Agrega más según necesites
        mkdir -p notebooks_to_deploy
        for nb in "${NOTEBOOKS[@]}"; do
          echo "Exportando $nb en modo raw..."
          curl -s -X GET \
            -H "Authorization: Bearer $ORIGIN_TOKEN" \
            "$ORIGIN_HOST/api/2.0/workspace/export?path=$NOTEBOOK_BASE/$nb&format=SOURCE&direct_download=true" \
            --output "notebooks_to_deploy/$nb.py"
        done
    
    - name: Deploy notebooks to Destination Workspace
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        DEST_BASE="/py/scripts/main"
        for file in notebooks_to_deploy/*.py; do
          name=$(basename "$file" .py)
          dest_path="$DEST_BASE/$name"
          echo "Creando carpeta $DEST_BASE si no existe..."
          curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"path\":\"$DEST_BASE\"}" \
            "$DEST_HOST/api/2.0/workspace/mkdirs"
          echo "Importando $file → $dest_path"
          response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: multipart/form-data" \
            -F "path=$dest_path" \
            -F "format=SOURCE" \
            -F "language=PYTHON" \
            -F "overwrite=true" \
            -F "content=@$file" \
            "$DEST_HOST/api/2.0/workspace/import")
          echo "Response: $response"
        done
    
    - name: Check if workflow exists and delete if necessary
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        WORKFLOW_NAME="WF_ADB"
        
        echo "Verificando si existe el workflow: $WORKFLOW_NAME"
        
        # Listar todos los workflows y buscar por nombre
        workflows_response=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.1/jobs/list")
        
        # Extraer job_id si existe el workflow
        existing_job_id=$(echo "$workflows_response" | jq -r --arg name "$WORKFLOW_NAME" '.jobs[]? | select(.settings.name == $name) | .job_id')
        
        if [ "$existing_job_id" != "" ] && [ "$existing_job_id" != "null" ]; then
          echo "Workflow encontrado con ID: $existing_job_id. Eliminando..."
          delete_response=$(curl -s -X POST \
            -H "Authorization: Bearer $DEST_TOKEN" \
            -H "Content-Type: application/json" \
            -d "{\"job_id\": $existing_job_id}" \
            "$DEST_HOST/api/2.1/jobs/delete")
          echo "Delete response: $delete_response"
        else
          echo "No se encontró workflow existente con nombre: $WORKFLOW_NAME"
        fi
    
    - name: Create Databricks Workflow WF_ADB
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        DEST_BASE="/py/scripts/main"
        
        echo "Creando workflow: WF_ADB"
        
        # Crear el JSON del workflow
        cat > workflow_config.json << 'EOF'
        {
          "name": "WF_ADB",
          "format": "MULTI_TASK",
          "tasks": [
            {
              "task_key": "tarea1_notebook1",
              "description": "Ejecuta notebook ntbk_1",
              "notebook_task": {
                "notebook_path": "/py/scripts/main/ntbk_1",
                "source": "WORKSPACE"
              },
                "compute": {
                  "serverless_compute": {}
                },
              "timeout_seconds": 3600,
              "max_retries": 2
            },
            {
              "task_key": "tarea2_notebook2",
              "description": "Ejecuta notebook ntbk_2",
              "notebook_task": {
                "notebook_path": "/py/scripts/main/ntbk_2",
                "source": "WORKSPACE"
              },
                "compute": {
                  "serverless_compute": {}
                },
              "timeout_seconds": 3600,
              "max_retries": 2,
              "depends_on": [
                  { "task_key": "tarea1_notebook1" }
              ]
            }
          ],
          "schedule": {
            "quartz_cron_expression": "0 0 8 * * ?",
            "timezone_id": "America/Lima",
            "pause_status": "PAUSED"
          },
          "email_notifications": {
            "on_failure": [],
            "on_success": [],
            "no_alert_for_skipped_runs": false
          },
          "webhook_notifications": {},
          "timeout_seconds": 7200,
          "max_concurrent_runs": 1,
          "tags": {
            "environment": "production",
            "created_by": "github_actions",
            "project": "automated_deployment"
          }
        }
        EOF
        
        # Crear el workflow
        create_response=$(curl -s -X POST \
          -H "Authorization: Bearer $DEST_TOKEN" \
          -H "Content-Type: application/json" \
          -d @workflow_config.json \
            "$DEST_HOST/api/2.2/jobs/create")
        
        echo "Workflow creation response: $create_response"
        
        # Extraer job_id del response
        job_id=$(echo "$create_response" | jq -r '.job_id')
        
        if [ "$job_id" != "" ] && [ "$job_id" != "null" ]; then
            echo "Workflow 'WF_ADB' creado con ID: $job_id"
        else
          echo "Error al crear el workflow"
          exit 1
        fi
    
    - name: Validate Workflow Configuration
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        WORKFLOW_NAME="WF_ADB"
        
        echo "Validando la configuración del workflow creado..."
        
        # Obtener lista de workflows y encontrar el recién creado
        workflows_list=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.2/jobs/list")
        
          job_id=$(echo "$workflows_list" | jq -r --arg name "$WORKFLOW_NAME" \
            '.jobs[]? | select(.settings.name == $name) | .job_id')
        
        if [ "$job_id" != "" ] && [ "$job_id" != "null" ]; then
          echo "Workflow encontrado con ID: $job_id"
          
          # Obtener configuración detallada
          job_details=$(curl -s -X GET \
            -H "Authorization: Bearer $DEST_TOKEN" \
            "$DEST_HOST/api/2.1/jobs/get?job_id=$job_id")
          
          echo "Resumen del workflow:"
          echo "Nombre: $(echo "$job_details" | jq -r '.settings.name')"
          echo "Número de tareas: $(echo "$job_details" | jq '.settings.tasks | length')"
          echo ""
          echo "Tareas configuradas:"
          echo "$job_details" | jq -r '.settings.tasks[] | "- " + .task_key + " → " + .notebook_task.notebook_path'
          echo ""
            echo "Compute configurado: Serverless Compute"
        else
          echo "No se pudo encontrar el workflow creado"
          exit 1
        fi
      
    - name: Run Databricks Job WF_ADB
      run: |
        DEST_HOST=${{ secrets.DATABRICKS_DEST_HOST }}
        DEST_TOKEN=${{ secrets.DATABRICKS_DEST_TOKEN }}
        JOB_NAME="WF_ADB"

        # Recupera el job_id por nombre
        JOB_ID=$(curl -s -X GET \
          -H "Authorization: Bearer $DEST_TOKEN" \
          "$DEST_HOST/api/2.2/jobs/list" \
          | jq -r --arg name "$JOB_NAME" \
            '.jobs[]? | select(.settings.name == $name) | .job_id')

        if [ -z "$JOB_ID" ] || [ "$JOB_ID" == "null" ]; then
          echo "❌ No pude encontrar el job '$JOB_NAME'"
          exit 1
        fi

        echo "▶️ Ejecutando job '$JOB_NAME' (ID: $JOB_ID)..."

        # Lanza la ejecución
        run_response=$(curl -s -X POST \
          -H "Authorization: Bearer $DEST_TOKEN" \
          -H "Content-Type: application/json" \
          -d "{\"job_id\": $JOB_ID}" \
          "$DEST_HOST/api/2.2/jobs/run-now")

        echo "Run-now response: $run_response"
        RUN_ID=$(echo "$run_response" | jq -r '.run_id')

        if [ "$RUN_ID" != "null" ] && [ -n "$RUN_ID" ]; then
          echo "✅ Run iniciado con run_id: $RUN_ID"
        else
          echo "❌ Falló el lanzamiento del run"
          exit 1
        fi
    
    - name: Clean up
      run: |
        rm -rf notebooks_to_deploy
        rm -f workflow_config.json
    
    - name: Done
      run: |
        echo "¡Despliegue completado exitosamente!"
        echo ""
        echo "Resumen:"
        echo "Notebooks desplegados: ntbk_1, ntbk_2"
        echo "Workflow creado: WF_ADB"
        echo "Tareas configuradas:"
        echo "   - tarea1_notebook1 (ntbk_1)"
        echo "   - tarea2_notebook2 (ntbk_2)"
          echo "Compute: Serverless Compute"
        echo ""
        echo "Accede a tu workspace de Databricks para ver el workflow en la sección 'Workflows'"